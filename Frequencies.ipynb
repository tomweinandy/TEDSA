{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Load in data\n",
    "admissions = 'tedsa_puf_2019.csv'\n",
    "df_raw = pd.read_csv(f'../../Downloads/{admissions}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter out select rows and columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5 columns (57 remain)\n",
      "Dropped 1,340,233 observations or 71.9% of the data (524,134 rows remain)\n"
     ]
    }
   ],
   "source": [
    "# Get count of original number of rows\n",
    "old_rows = len(df_raw)\n",
    "\n",
    "# Drop defined columns (year of admission, case id, geographic metro area, geographic division, geographic region)\n",
    "columns_to_drop = ['ADMYR', 'CASEID', 'CBSA2010', 'DIVISION', 'REGION']\n",
    "df = df_raw.drop(columns=columns_to_drop)\n",
    "print(f'Dropped {len(columns_to_drop)} columns ({len(df.columns)} remain)')\n",
    "\n",
    "# Drop values where dependent variable is unknown\n",
    "df = df[df['METHUSE'] != -9]\n",
    "\n",
    "# Only keep patients admitted with self-described use of an opioid as their primary substance use (i.e., SUB1 = 5, 6, or 7)\n",
    "df = df[df['SUB1'].between(5, 7)]\n",
    "new_rows = len(df)\n",
    "percent_change = round(100*(old_rows-new_rows)/old_rows, 1)\n",
    "print(f'Dropped {\"{:,}\".format(old_rows-new_rows)} observations or {percent_change}% of the data ({\"{:,}\".format(new_rows)} rows remain)')\n",
    "\n",
    "df = df.reset_index(drop='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make dataset human-readable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load in variable dictionary\n",
    "with open('VariableDictionary.txt') as file:\n",
    "    variable_dict_string = file.read()\n",
    "    variable_dict = ast.literal_eval(variable_dict_string)\n",
    "\n",
    "# Rename entries in column according to dictionary\n",
    "df2 = df.copy()\n",
    "for col, col_dict in variable_dict.items():\n",
    "    if col in df2.columns:\n",
    "        for old_value, new_value in variable_dict[col].items():\n",
    "            df2[col] = df2[col].replace(old_value, new_value)\n",
    "\n",
    "# Rename \"-9\" values as \"Unknown\"\n",
    "for col in df2.columns:\n",
    "    df2[col] = df2[col].replace(-9, 'Unknown')\n",
    "\n",
    "# Merge DETNLF (detailed not in labor force) into EMPLOY==4 (not in labor force)\n",
    "detailed_employ = []\n",
    "\n",
    "for idx, value in df2.iterrows():\n",
    "    if value['EMPLOY'] == 'NotInLaborForce':\n",
    "        if value['DETNLF'] == 'Unknown':\n",
    "            # Assign 'UnknownNotInLaborForce' if 'NotInLaborForce' and 'Unknown'\n",
    "            detailed_employ.append('UnknownNotInLaborForce')\n",
    "        else:\n",
    "            # Otherwise, assign as the DETNLF value\n",
    "            detailed_employ.append(value['DETNLF'])\n",
    "    else:\n",
    "        # Assign the EMPLOY value if not 'NotInLaborForce'\n",
    "        detailed_employ.append(value['EMPLOY'])\n",
    "\n",
    "# Add a new column for detailed employment and drop the two source columns\n",
    "df2['DETEMPLOY'] = detailed_employ\n",
    "df2 = df2.drop(columns=['EMPLOY', 'DETNLF'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describe MOUD treatment frequency by state and living status\n",
    "Calculate the percent difference between the two homeless/non-homeless and statistical difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     Homeless    Housed  PercentDifference      TStat    PValue Significance\nAK   0.387387  0.518122          -0.252324  -4.249484  0.000023         ****\nAL   0.118734  0.312766          -0.620376  -8.004596  0.000000         ****\nAR   0.103448  0.364159          -0.715925  -7.541382  0.000000         ****\nCA   0.344436  0.669338          -0.485408 -61.263606  0.000000         ****\nCO   0.364320  0.366509          -0.005972  -0.181478  0.855995             \nCT   0.176840  0.333963          -0.470480 -17.040277  0.000000         ****\nDC   0.079659  0.057485           0.385728   1.725765  0.084591            *\nDE   0.104848  0.253067          -0.585691  -9.880705  0.000000         ****\nGA   0.207951  0.236675          -0.121363  -1.155031  0.248175             \nHI   0.060606  0.072626          -0.165501  -0.327188  0.743808             \nIA   0.281301  0.387677          -0.274393  -4.681897  0.000003         ****\nID   0.000000  0.023529          -1.000000  -0.532160  0.595857             \nIL   0.062088  0.102272          -0.392908  -5.841952  0.000000         ****\nIN   0.240000  0.374981          -0.359968  -6.199392  0.000000         ****\nKS   0.125000  0.208835          -0.401442  -1.235859  0.217522             \nKY   0.078405  0.306334          -0.744053 -18.254217  0.000000         ****\nLA   0.013029  0.019440          -0.329757  -0.768877  0.442054             \nMA   0.216281  0.304790          -0.290394 -18.944718  0.000000         ****\nMD   0.161132  0.524263          -0.692650 -66.669003  0.000000         ****\nME   0.753223  0.765075          -0.015492  -0.600491  0.548215             \nMI   0.522946  0.580268          -0.098785  -8.003398  0.000000         ****\nMN   0.560697  0.526443           0.065067   2.556019  0.010602           **\nMO   0.119259  0.153536          -0.223251  -3.137250  0.001711          ***\nMS   0.011364  0.022648          -0.498252  -0.697494  0.485625             \nMT   0.000000  0.000000                NaN        NaN       NaN             \nNC   0.076720  0.141904          -0.459354  -5.059808  0.000000         ****\nND   0.187500  0.165217           0.134868   0.221732  0.824873             \nNE   0.122449  0.343137          -0.643149  -3.127550  0.001909          ***\nNH   0.212121  0.198561           0.068292   0.560195  0.575455             \nNJ   0.685193  0.682183           0.004413   0.295288  0.767776             \nNM   0.901099  0.807537           0.115861   2.185773  0.029110           **\nNV   0.333333  0.217304           0.533951   1.727083  0.084728            *\nNY   0.346714  0.497272          -0.302767 -36.693141  0.000000         ****\nOH   0.383109  0.310937           0.232111   4.203492  0.000027         ****\nOK   0.000000  0.000000                NaN        NaN       NaN             \nPA   0.339956  0.370370          -0.082119  -1.810339  0.070273            *\nPR   0.158273  0.275794          -0.426117  -2.964223  0.003097          ***\nRI   0.445561  0.701554          -0.364894 -12.808389  0.000000         ****\nSC   0.600000  0.492537           0.218182   0.457800  0.648514             \nSD   0.000000  0.283019          -1.000000  -2.873357  0.004236          ***\nTN   0.005724  0.004804           0.191333   0.415806  0.677564             \nTX   0.093907  0.299862          -0.686832 -17.924876  0.000000         ****\nUT   0.160550  0.307388          -0.477694 -10.823623  0.000000         ****\nVA   0.000000  0.000000                NaN        NaN       NaN             \nVT   0.668998  0.720878          -0.071969  -2.208600  0.027275           **\nWI   0.051107  0.141311          -0.638334  -6.132794  0.000000         ****\nWY   0.111111  0.040453           1.746667   1.765108  0.078020            *\nUSA  0.285596  0.428059          -0.332811 -77.837711  0.000000         ****",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Homeless</th>\n      <th>Housed</th>\n      <th>PercentDifference</th>\n      <th>TStat</th>\n      <th>PValue</th>\n      <th>Significance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AK</th>\n      <td>0.387387</td>\n      <td>0.518122</td>\n      <td>-0.252324</td>\n      <td>-4.249484</td>\n      <td>0.000023</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>AL</th>\n      <td>0.118734</td>\n      <td>0.312766</td>\n      <td>-0.620376</td>\n      <td>-8.004596</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>AR</th>\n      <td>0.103448</td>\n      <td>0.364159</td>\n      <td>-0.715925</td>\n      <td>-7.541382</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>CA</th>\n      <td>0.344436</td>\n      <td>0.669338</td>\n      <td>-0.485408</td>\n      <td>-61.263606</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>CO</th>\n      <td>0.364320</td>\n      <td>0.366509</td>\n      <td>-0.005972</td>\n      <td>-0.181478</td>\n      <td>0.855995</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>CT</th>\n      <td>0.176840</td>\n      <td>0.333963</td>\n      <td>-0.470480</td>\n      <td>-17.040277</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>DC</th>\n      <td>0.079659</td>\n      <td>0.057485</td>\n      <td>0.385728</td>\n      <td>1.725765</td>\n      <td>0.084591</td>\n      <td>*</td>\n    </tr>\n    <tr>\n      <th>DE</th>\n      <td>0.104848</td>\n      <td>0.253067</td>\n      <td>-0.585691</td>\n      <td>-9.880705</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>GA</th>\n      <td>0.207951</td>\n      <td>0.236675</td>\n      <td>-0.121363</td>\n      <td>-1.155031</td>\n      <td>0.248175</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>HI</th>\n      <td>0.060606</td>\n      <td>0.072626</td>\n      <td>-0.165501</td>\n      <td>-0.327188</td>\n      <td>0.743808</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>IA</th>\n      <td>0.281301</td>\n      <td>0.387677</td>\n      <td>-0.274393</td>\n      <td>-4.681897</td>\n      <td>0.000003</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <td>0.000000</td>\n      <td>0.023529</td>\n      <td>-1.000000</td>\n      <td>-0.532160</td>\n      <td>0.595857</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>IL</th>\n      <td>0.062088</td>\n      <td>0.102272</td>\n      <td>-0.392908</td>\n      <td>-5.841952</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>IN</th>\n      <td>0.240000</td>\n      <td>0.374981</td>\n      <td>-0.359968</td>\n      <td>-6.199392</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>KS</th>\n      <td>0.125000</td>\n      <td>0.208835</td>\n      <td>-0.401442</td>\n      <td>-1.235859</td>\n      <td>0.217522</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>KY</th>\n      <td>0.078405</td>\n      <td>0.306334</td>\n      <td>-0.744053</td>\n      <td>-18.254217</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>LA</th>\n      <td>0.013029</td>\n      <td>0.019440</td>\n      <td>-0.329757</td>\n      <td>-0.768877</td>\n      <td>0.442054</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>MA</th>\n      <td>0.216281</td>\n      <td>0.304790</td>\n      <td>-0.290394</td>\n      <td>-18.944718</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>MD</th>\n      <td>0.161132</td>\n      <td>0.524263</td>\n      <td>-0.692650</td>\n      <td>-66.669003</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>ME</th>\n      <td>0.753223</td>\n      <td>0.765075</td>\n      <td>-0.015492</td>\n      <td>-0.600491</td>\n      <td>0.548215</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>MI</th>\n      <td>0.522946</td>\n      <td>0.580268</td>\n      <td>-0.098785</td>\n      <td>-8.003398</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>MN</th>\n      <td>0.560697</td>\n      <td>0.526443</td>\n      <td>0.065067</td>\n      <td>2.556019</td>\n      <td>0.010602</td>\n      <td>**</td>\n    </tr>\n    <tr>\n      <th>MO</th>\n      <td>0.119259</td>\n      <td>0.153536</td>\n      <td>-0.223251</td>\n      <td>-3.137250</td>\n      <td>0.001711</td>\n      <td>***</td>\n    </tr>\n    <tr>\n      <th>MS</th>\n      <td>0.011364</td>\n      <td>0.022648</td>\n      <td>-0.498252</td>\n      <td>-0.697494</td>\n      <td>0.485625</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>MT</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>NC</th>\n      <td>0.076720</td>\n      <td>0.141904</td>\n      <td>-0.459354</td>\n      <td>-5.059808</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>ND</th>\n      <td>0.187500</td>\n      <td>0.165217</td>\n      <td>0.134868</td>\n      <td>0.221732</td>\n      <td>0.824873</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>NE</th>\n      <td>0.122449</td>\n      <td>0.343137</td>\n      <td>-0.643149</td>\n      <td>-3.127550</td>\n      <td>0.001909</td>\n      <td>***</td>\n    </tr>\n    <tr>\n      <th>NH</th>\n      <td>0.212121</td>\n      <td>0.198561</td>\n      <td>0.068292</td>\n      <td>0.560195</td>\n      <td>0.575455</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>NJ</th>\n      <td>0.685193</td>\n      <td>0.682183</td>\n      <td>0.004413</td>\n      <td>0.295288</td>\n      <td>0.767776</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>NM</th>\n      <td>0.901099</td>\n      <td>0.807537</td>\n      <td>0.115861</td>\n      <td>2.185773</td>\n      <td>0.029110</td>\n      <td>**</td>\n    </tr>\n    <tr>\n      <th>NV</th>\n      <td>0.333333</td>\n      <td>0.217304</td>\n      <td>0.533951</td>\n      <td>1.727083</td>\n      <td>0.084728</td>\n      <td>*</td>\n    </tr>\n    <tr>\n      <th>NY</th>\n      <td>0.346714</td>\n      <td>0.497272</td>\n      <td>-0.302767</td>\n      <td>-36.693141</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>OH</th>\n      <td>0.383109</td>\n      <td>0.310937</td>\n      <td>0.232111</td>\n      <td>4.203492</td>\n      <td>0.000027</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>OK</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>PA</th>\n      <td>0.339956</td>\n      <td>0.370370</td>\n      <td>-0.082119</td>\n      <td>-1.810339</td>\n      <td>0.070273</td>\n      <td>*</td>\n    </tr>\n    <tr>\n      <th>PR</th>\n      <td>0.158273</td>\n      <td>0.275794</td>\n      <td>-0.426117</td>\n      <td>-2.964223</td>\n      <td>0.003097</td>\n      <td>***</td>\n    </tr>\n    <tr>\n      <th>RI</th>\n      <td>0.445561</td>\n      <td>0.701554</td>\n      <td>-0.364894</td>\n      <td>-12.808389</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>SC</th>\n      <td>0.600000</td>\n      <td>0.492537</td>\n      <td>0.218182</td>\n      <td>0.457800</td>\n      <td>0.648514</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>SD</th>\n      <td>0.000000</td>\n      <td>0.283019</td>\n      <td>-1.000000</td>\n      <td>-2.873357</td>\n      <td>0.004236</td>\n      <td>***</td>\n    </tr>\n    <tr>\n      <th>TN</th>\n      <td>0.005724</td>\n      <td>0.004804</td>\n      <td>0.191333</td>\n      <td>0.415806</td>\n      <td>0.677564</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>TX</th>\n      <td>0.093907</td>\n      <td>0.299862</td>\n      <td>-0.686832</td>\n      <td>-17.924876</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>UT</th>\n      <td>0.160550</td>\n      <td>0.307388</td>\n      <td>-0.477694</td>\n      <td>-10.823623</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>VA</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>VT</th>\n      <td>0.668998</td>\n      <td>0.720878</td>\n      <td>-0.071969</td>\n      <td>-2.208600</td>\n      <td>0.027275</td>\n      <td>**</td>\n    </tr>\n    <tr>\n      <th>WI</th>\n      <td>0.051107</td>\n      <td>0.141311</td>\n      <td>-0.638334</td>\n      <td>-6.132794</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n    <tr>\n      <th>WY</th>\n      <td>0.111111</td>\n      <td>0.040453</td>\n      <td>1.746667</td>\n      <td>1.765108</td>\n      <td>0.078020</td>\n      <td>*</td>\n    </tr>\n    <tr>\n      <th>USA</th>\n      <td>0.285596</td>\n      <td>0.428059</td>\n      <td>-0.332811</td>\n      <td>-77.837711</td>\n      <td>0.000000</td>\n      <td>****</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Simplify dataset\n",
    "df_ttest = df2[['STFIPS', 'LIVARAG', 'METHUSE']]\n",
    "df_ttest = df_ttest[df_ttest['LIVARAG'] != 'Unknown']\n",
    "# df_ttest = df_ttest[df_ttest['LIVARAG'] != 'DependLiving']\n",
    "df_ttest['LIVARAG'] = df_ttest['LIVARAG'].replace({'DependLiving':'Housed', 'IndependentLiving':'Housed'})\n",
    "df_ttest['METHUSE'] = df_ttest['METHUSE'].replace({'MethUse':1, 'NoMethUse':0})\n",
    "\n",
    "# Create loop assets\n",
    "state_array = df_ttest['STFIPS'].sort_values().unique()\n",
    "state_list = state_array.tolist()\n",
    "state_list.append('USA')\n",
    "df_state = pd.DataFrame(columns=['Homeless', 'Housed', 'PercentDifference', 'TStat', 'PValue', 'Significance'])\n",
    "\n",
    "# Loop through each state\n",
    "for s in state_list:\n",
    "    # Define dataframe based on whether state or national\n",
    "    if s == 'USA':\n",
    "        dft_temp = df_ttest.copy()\n",
    "    else:\n",
    "        dft_temp = df_ttest[df_ttest['STFIPS'] == s]\n",
    "\n",
    "    # Group by living arrangement and MOUD, then store those variables for later (if they exist)\n",
    "    dft_grouped = dft_temp.groupby('LIVARAG')['METHUSE'].mean()\n",
    "    homeless_moud = dft_grouped.loc['Homeless']\n",
    "    independent_moud = dft_grouped.loc['Housed']\n",
    "\n",
    "    # Calculate the percent difference (if there is a homeless group in that state)\n",
    "    if independent_moud > 0:\n",
    "       percent_difference = (homeless_moud - independent_moud)/independent_moud\n",
    "    # elif homeless_moud = 0:\n",
    "    #     percent_difference\n",
    "    else:\n",
    "        percent_difference = np.nan\n",
    "\n",
    "    # Perform a t-test between the two groups\n",
    "    group_A_values = dft_temp[dft_temp['LIVARAG'] == 'Homeless']['METHUSE']\n",
    "    group_B_values = dft_temp[dft_temp['LIVARAG'] == 'Housed']['METHUSE']\n",
    "    t_stat, p_value = ttest_ind(group_A_values, group_B_values)\n",
    "\n",
    "    # Add significance based on p-value\n",
    "    if p_value < 0.001:\n",
    "        significance = '****'\n",
    "    elif p_value < 0.01:\n",
    "        significance = '***'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '**'\n",
    "    elif p_value <0.1:\n",
    "        significance = '*'\n",
    "    else:\n",
    "        significance = ''\n",
    "\n",
    "    # Add values to a dictionary and round\n",
    "    dict_results = {'Homeless':homeless_moud, 'Housed':independent_moud, 'PercentDifference':percent_difference, 'TStat':t_stat, 'PValue':p_value}\n",
    "    for key, value in dict_results.items():\n",
    "        dict_results[key] = round(value, 6)\n",
    "\n",
    "    # Add significance and to dictionary and then dictionary to dataframe\n",
    "    dict_results['Significance'] = significance\n",
    "    df_state.loc[s] = dict_results\n",
    "\n",
    "# df_state.to_csv('frequencies.csv') #uncomment to save file\n",
    "df_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}